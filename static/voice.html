<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>üé§ Voice Assistant</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            background: #f6f7fa;
            margin: 0;
            min-height: 100vh;
        }

        .container {
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: flex-start;
            padding: 20px 10px;
        }

        .chat-window {
            width: 100%;
            max-width: 480px;
            height: 500px;
            background: #fff;
            border-radius: 22px;
            box-shadow: 0 2px 12px rgba(0, 0, 0, 0.08);
            display: flex;
            flex-direction: column;
            overflow: hidden;
            border: 1.5px solid #e0e6ed;
        }

        .chat-header {
            background: #4a90e2;
            color: #fff;
            padding: 22px 0 18px 0;
            text-align: center;
            font-size: 1.5rem;
            font-weight: bold;
            border-radius: 22px 22px 0 0;
            letter-spacing: 0.5px;
        }

        .chat-messages {
            flex: 1;
            padding: 20px;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
            gap: 18px;
            background: #fcfdff;
        }

        .message {
            padding: 18px 20px;
            border-radius: 16px;
            max-width: 85%;
            line-height: 1.6;
            font-size: 16px;
            word-break: break-word;
            background: #fff;
            border: 2px solid #4a90e2;
            color: #222;
        }

        .message.user {
            background: #f5faff;
            color: #222;
            align-self: flex-end;
            border: 2px solid #4a90e2;
        }

        .message.assistant {
            background: #fff;
            color: #222;
            align-self: flex-start;
            border: 2px solid #4a90e2;
        }

        .status-bar {
            background: #fff7f0;
            color: #666;
            font-size: 14px;
            text-align: center;
            padding: 8px 12px;
            border-top: 1px solid #f0f0f0;
            border-bottom: 1px solid #f0f0f0;
        }

        .bottom-bar {
            background: #fff;
            padding: 12px 16px 16px 16px;
            border-radius: 0 0 22px 22px;
        }

        .input-bar {
            width: 100%;
            background: #f8f9fb;
            border: 1.5px solid #e0e6ed;
            border-radius: 24px;
            display: flex;
            align-items: center;
            padding: 0 14px 0 0px;
            gap: 8px;
            transition: border-color 0.2s;
        }

        .input-bar:focus-within {
            border-color: #4a90e2;
        }

        .input-box {
            flex: 1;
            border: none;
            background: transparent;
            font-size: 17px;
            padding: 16px 0;
            outline: none;
        }

        .mic-btn,
        .stop-btn {
            width: 36px;
            height: 36px;
            border-radius: 50%;
            background: #4a90e2;
            color: #fff;
            border: none;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 18px;
            padding: 0;
            cursor: pointer;
            transition: all 0.2s;
            flex-shrink: 0;
        }

        .mic-btn:hover {
            background: #3a7bc8;
            transform: scale(1.05);
        }

        .mic-btn.recording {
            background: #f43f5e;
            animation: pulse 1.5s infinite;
        }

        .stop-btn {
            background: #ef4444;
        }

        .stop-btn:hover {
            background: #dc2626;
            transform: scale(1.05);
        }

        @keyframes pulse {

            0%,
            100% {
                opacity: 1;
            }

            50% {
                opacity: 0.7;
            }
        }

        .mic-btn.interrupt-ready {
            background: #f59e0b !important;
            animation: pulse-interrupt 1.5s infinite !important;
        }

        @keyframes pulse-interrupt {

            0%,
            100% {
                opacity: 1;
                box-shadow: 0 0 0 0 rgba(245, 158, 11, 0.7);
            }

            50% {
                opacity: 0.9;
                box-shadow: 0 0 0 10px rgba(245, 158, 11, 0);
            }
        }

        @media (max-width: 600px) {
            .chat-window {
                max-width: 100vw;
                height: calc(100vh - 40px);
                border-radius: 16px;
            }

            .chat-header {
                border-radius: 16px 16px 0 0;
            }

            .bottom-bar {
                border-radius: 0 0 16px 16px;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="chat-window">
            <div class="chat-header">Voice Assistant</div>
            <div class="chat-messages" id="chat-messages"></div>
            <audio id="botAudio" autoplay></audio>
            <div class="status-bar" id="status">Click mic to start</div>
            <div class="bottom-bar">
                <div class="input-bar">
                    <input id="textInput" class="input-box" type="text" placeholder="Ask anything..."
                        autocomplete="off" />
                    <button id="micBtn" class="mic-btn" title="Start Recording">üéôÔ∏è</button>
                    <button id="stopBtn" class="stop-btn" title="Stop Recording" style="display:none;">‚èπÔ∏è</button>
                </div>
            </div>
        </div>
    </div>

    <script>
        const statusDiv = document.getElementById('status');
        const chatMessages = document.getElementById('chat-messages');
        const micBtn = document.getElementById('micBtn');
        const stopBtn = document.getElementById('stopBtn');
        const botAudio = document.getElementById('botAudio');

        let ws = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let sessionId = null;
        let currentStream = null;
        let currentAudioContext = null;
        let isProcessingQuery = false;
        let isBotSpeaking = false;
        let isWaitingForResponse = false;
        let silenceInterval = null;
        let isManualStop = false;
        let voiceInterruptionStream = null;
        let voiceInterruptionContext = null;
        let voiceInterruptionAnalyser = null;
        let voiceInterruptionInterval = null;

        function addMessage(text, sender) {
            const messageElem = document.createElement('div');
            messageElem.classList.add('message', sender);
            messageElem.textContent = text;
            chatMessages.appendChild(messageElem);
            chatMessages.scrollTop = chatMessages.scrollHeight;
        }

        function speakText(text) {
            if (!text || text.trim() === '') return;
            window.speechSynthesis.cancel();
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.rate = 1.0;
            utterance.pitch = 1.0;
            utterance.volume = 1.0;
            window.speechSynthesis.speak(utterance);
        }

        async function startVoiceInterruptionDetection() {
            if (voiceInterruptionStream) {
                console.log("Voice interruption detection already running");
                return;
            }

            try {
                console.log("Starting background voice detection for interruption...");

                voiceInterruptionStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        channelCount: 1,
                        sampleRate: 44100
                    }
                });

                voiceInterruptionContext = new AudioContext();
                const source = voiceInterruptionContext.createMediaStreamSource(voiceInterruptionStream);

                voiceInterruptionAnalyser = voiceInterruptionContext.createAnalyser();
                voiceInterruptionAnalyser.fftSize = 2048;
                voiceInterruptionAnalyser.smoothingTimeConstant = 0.8;
                source.connect(voiceInterruptionAnalyser);

                const bufferLength = voiceInterruptionAnalyser.frequencyBinCount;
                const dataArray = new Float32Array(bufferLength);
                const frequencyData = new Uint8Array(bufferLength);

                let consecutiveVoiceDetections = 0;

                function detectVoiceInterruption() {
                    if (!isBotSpeaking || !voiceInterruptionAnalyser || isManualStop) {
                        return;
                    }

                    voiceInterruptionAnalyser.getFloatTimeDomainData(dataArray);
                    voiceInterruptionAnalyser.getByteFrequencyData(frequencyData);

                    const rms = Math.sqrt(dataArray.reduce((sum, v) => sum + v * v, 0) / bufferLength);
                    const db = 20 * Math.log10(rms + 1e-8);

                    const sampleRate = voiceInterruptionContext.sampleRate;
                    const voiceFreqStart = Math.floor(300 * bufferLength / (sampleRate / 2));
                    const voiceFreqEnd = Math.floor(3400 * bufferLength / (sampleRate / 2));
                    const midFreqStart = Math.floor(800 * bufferLength / (sampleRate / 2));
                    const midFreqEnd = Math.floor(2000 * bufferLength / (sampleRate / 2));

                    let voiceEnergy = 0;
                    let totalEnergy = 0;
                    let midFreqEnergy = 0;

                    for (let i = 0; i < bufferLength; i++) {
                        const energy = frequencyData[i];
                        totalEnergy += energy;

                        if (i >= voiceFreqStart && i <= voiceFreqEnd) {
                            voiceEnergy += energy;
                        }
                        if (i >= midFreqStart && i <= midFreqEnd) {
                            midFreqEnergy += energy;
                        }
                    }

                    const voiceRatio = totalEnergy > 0 ? voiceEnergy / totalEnergy : 0;
                    const midFreqRatio = totalEnergy > 0 ? midFreqEnergy / totalEnergy : 0;

                    // Balanced thresholds - responsive but not too sensitive
                    const isVoiceDetected =
                        db > -30 &&              // Moderate dB threshold
                        voiceRatio > 0.15 &&     // Moderate voice frequency ratio
                        midFreqRatio > 0.10 &&   // Moderate mid-frequency ratio
                        totalEnergy > 35;        // Moderate total energy

                    if (isVoiceDetected) {
                        consecutiveVoiceDetections++;
                        // Log every 3rd detection for visibility
                        if (consecutiveVoiceDetections % 3 === 0) {
                            console.log(`üé§ Voice detected: dB=${db.toFixed(1)}, count=${consecutiveVoiceDetections}`);
                        }

                        // Require 3 consecutive detections (600ms of speech)
                        if (consecutiveVoiceDetections >= 3) {
                            console.log("üéôÔ∏è Voice interruption confirmed!");
                            handleVoiceInterruption();
                        }
                    } else {
                        if (consecutiveVoiceDetections > 0) {
                            consecutiveVoiceDetections = Math.max(0, consecutiveVoiceDetections - 1);
                        }
                    }
                }

                voiceInterruptionInterval = setInterval(detectVoiceInterruption, 200);
                // Silent start - no console log

            } catch (error) {
                console.error("Failed to start voice interruption detection:", error);
            }
        }

        function stopVoiceInterruptionDetection() {
            if (voiceInterruptionInterval) {
                clearInterval(voiceInterruptionInterval);
                voiceInterruptionInterval = null;
            }

            if (voiceInterruptionStream) {
                voiceInterruptionStream.getTracks().forEach(track => {
                    track.stop();
                });
                voiceInterruptionStream = null;
            }

            if (voiceInterruptionContext && voiceInterruptionContext.state !== 'closed') {
                voiceInterruptionContext.close();
                voiceInterruptionContext = null;
            }

            voiceInterruptionAnalyser = null;
        }

        function handleVoiceInterruption() {
            console.log("========== VOICE INTERRUPTION DETECTED ==========");

            // IMMEDIATELY clear processing flag to prevent blocking
            isProcessingQuery = false;
            isBotSpeaking = false;
            console.log("‚úÖ Cleared isProcessingQuery and isBotSpeaking flags");

            // Stop background voice interruption detection first
            stopVoiceInterruptionDetection();

            // Stop bot audio immediately
            if (botAudio && !botAudio.paused) {
                botAudio.pause();
                botAudio.currentTime = 0;
                console.log("Bot audio stopped");
            }

            // Stop speech synthesis if active
            window.speechSynthesis.cancel();

            // Remove visual indicators
            micBtn.classList.remove("interrupt-ready");
            micBtn.classList.remove("recording");

            // Fade last bot message
            const existingMessages = chatMessages.querySelectorAll('.message.assistant');
            if (existingMessages.length > 0) {
                const lastMessage = existingMessages[existingMessages.length - 1];
                lastMessage.style.opacity = '0.6';
            }

            // Cleanup any existing recording session completely
            console.log("Cleaning up existing recording session before interruption restart...");
            cleanupRecordingSession(false);

            if (!isManualStop) {
                statusDiv.textContent = "üéôÔ∏è Interrupted! Starting fresh recording...";
                statusDiv.className = "";

                // Longer delay to ensure complete cleanup
                setTimeout(() => {
                    if (ws && ws.readyState === WebSocket.OPEN && !isManualStop) {
                        console.log("Starting completely new recording after interruption");
                        startRecording();
                    } else {
                        console.log("Cannot restart - WebSocket not ready or manual stop active");
                    }
                }, 800); // Increased delay for better cleanup
            }
        }

        function cleanupRecordingSession(skipStopRecorder = false) {
            console.log("Cleaning up recording session...", "skipStopRecorder:", skipStopRecorder);

            // Stop voice interruption detection
            stopVoiceInterruptionDetection();

            // Stop interval first to prevent any more detectSilence calls
            if (silenceInterval) {
                clearInterval(silenceInterval);
                silenceInterval = null;
            }

            // Stop mediaRecorder unless we're doing a manual stop (to avoid triggering onstop)
            if (!skipStopRecorder && mediaRecorder && mediaRecorder.state !== "inactive") {
                try {
                    // CRITICAL: Remove onstop handler to prevent stale execution
                    const oldRecorder = mediaRecorder;
                    oldRecorder.onstop = null;
                    oldRecorder.stop();
                    console.log("‚úÖ Stopped old recorder with nullified onstop");
                } catch (e) {
                    console.log("Error stopping mediaRecorder:", e);
                }
            }

            // Stop all stream tracks
            if (currentStream) {
                currentStream.getTracks().forEach(track => {
                    track.stop();
                    console.log("Stopped track:", track.kind);
                });
                currentStream = null;
            }

            // Close audio context
            if (currentAudioContext && currentAudioContext.state !== 'closed') {
                currentAudioContext.close();
                currentAudioContext = null;
            }

            audioChunks = [];
            mediaRecorder = null;
        }

        async function startRecording() {
            console.log("startRecording called - isProcessingQuery:", isProcessingQuery, "isManualStop:", isManualStop);

            if (isManualStop) {
                console.log("Manual stop active, not starting recording");
                return;
            }

            cleanupRecordingSession(false);

            if (isBotSpeaking && botAudio) {
                console.log("Bot is speaking, handling interruption in startRecording...");

                // Stop voice interruption detection
                stopVoiceInterruptionDetection();

                // Stop bot audio
                botAudio.pause();
                botAudio.currentTime = 0;

                // Stop speech synthesis
                window.speechSynthesis.cancel();

                // Clear flags
                isBotSpeaking = false;
                isProcessingQuery = false;

                // Remove visual indicators
                micBtn.classList.remove("interrupt-ready");

                // Fade last message
                const existingMessages = chatMessages.querySelectorAll('.message.assistant');
                if (existingMessages.length > 0) {
                    const lastMessage = existingMessages[existingMessages.length - 1];
                    lastMessage.style.opacity = '0.6';
                }

                statusDiv.textContent = "üéôÔ∏è Interrupted. Starting fresh recording...";
                statusDiv.className = "";
                console.log("Bot speech interrupted, ready for fresh recording");
            }

            try {
                console.log("Getting new audio stream...");
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        channelCount: 1,
                        sampleRate: 44100
                    }
                });

                currentStream = stream;
                const audioContext = new AudioContext();
                currentAudioContext = audioContext;
                const source = audioContext.createMediaStreamSource(stream);

                const analyser = audioContext.createAnalyser();
                analyser.fftSize = 4096;
                analyser.smoothingTimeConstant = 0.8;
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Float32Array(bufferLength);
                const frequencyData = new Uint8Array(bufferLength);
                source.connect(analyser);

                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];
                let hasSound = false;
                let silenceCount = 0;
                isRecording = true; // Use global variable
                let silenceThreshold = 10; // 2 seconds (10 √ó 200ms)
                let lastSoundTime = Date.now();
                let voiceConfidenceCount = 0;

                function detectSilence() {
                    if (!isRecording || isManualStop) return;

                    analyser.getFloatTimeDomainData(dataArray);
                    analyser.getByteFrequencyData(frequencyData);

                    const rms = Math.sqrt(dataArray.reduce((sum, v) => sum + v * v, 0) / bufferLength);
                    const db = 20 * Math.log10(rms + 1e-8);

                    const voiceFreqStart = Math.floor(300 * bufferLength / (audioContext.sampleRate / 2));
                    const voiceFreqEnd = Math.floor(3400 * bufferLength / (audioContext.sampleRate / 2));
                    const midFreqStart = Math.floor(800 * bufferLength / (audioContext.sampleRate / 2));
                    const midFreqEnd = Math.floor(2000 * bufferLength / (audioContext.sampleRate / 2));

                    let voiceEnergy = 0;
                    let totalEnergy = 0;
                    let midFreqEnergy = 0;

                    for (let i = 0; i < bufferLength; i++) {
                        const energy = frequencyData[i];
                        totalEnergy += energy;
                        if (i >= voiceFreqStart && i <= voiceFreqEnd) {
                            voiceEnergy += energy;
                        }
                        if (i >= midFreqStart && i <= midFreqEnd) {
                            midFreqEnergy += energy;
                        }
                    }

                    const voiceRatio = totalEnergy > 0 ? voiceEnergy / totalEnergy : 0;
                    const midFreqRatio = totalEnergy > 0 ? midFreqEnergy / totalEnergy : 0;

                    const isLikelyVoice =
                        db > -30 && // Strong voice signal
                        voiceRatio > 0.15 &&
                        midFreqRatio > 0.08 &&
                        totalEnergy > 25;

                    // Fallback detection for initial voice (more lenient)
                    const isBasicVoice =
                        !hasSound && // Only for initial detection
                        db > -40 &&
                        voiceRatio > 0.10 &&
                        totalEnergy > 15;

                    // Emergency fallback for very quiet environments
                    const isFallbackVoice =
                        !hasSound &&
                        db > -45 &&
                        totalEnergy > 10;

                    if (isLikelyVoice || isBasicVoice || isFallbackVoice) {
                        voiceConfidenceCount++;
                        if (voiceConfidenceCount === 1) {
                            const method = isLikelyVoice ? 'strong' : (isBasicVoice ? 'basic' : 'fallback');
                            console.log(`‚úÖ Voice detected: dB=${db.toFixed(1)}, voiceRatio=${voiceRatio.toFixed(2)}, method=${method}`);
                        }

                        // Require confirmation for fallback voice
                        const requiredConfidence = isFallbackVoice ? 3 : (isBasicVoice ? 2 : 1);

                        if (voiceConfidenceCount >= requiredConfidence) {
                            hasSound = true;
                            silenceCount = 0;
                            lastSoundTime = Date.now();
                            if (!isManualStop) {
                                statusDiv.textContent = "üéôÔ∏è Listening... (Voice detected)";
                            }
                        }
                    } else {
                        // Faster decay for confidence to allow quicker silence detection
                        if (voiceConfidenceCount > 0) {
                            voiceConfidenceCount = Math.max(0, voiceConfidenceCount - 0.8); // Faster decay
                        }

                        // Only count as silence if confidence is low
                        if (voiceConfidenceCount < 0.5) {
                            silenceCount++;

                            // Show silence countdown
                            if (hasSound && silenceCount > 0) {
                                const remainingSilence = silenceThreshold - silenceCount;
                                const secondsLeft = Math.ceil(remainingSilence * 0.2);
                                if (!isManualStop && secondsLeft > 0) {
                                    statusDiv.textContent = `üîá Silence detected... ${secondsLeft}s`;
                                }
                            }
                        }

                        if (hasSound && silenceCount >= silenceThreshold) {
                            if (isManualStop) {
                                console.log("Manual stop active, not processing");
                                return;
                            }
                            console.log("üîá 2 seconds of silence detected - preparing to send audio");

                            // CRITICAL: Don't set isProcessingQuery yet - let onstop handler do it
                            isRecording = false;

                            if (silenceInterval) {
                                clearInterval(silenceInterval);
                                silenceInterval = null;
                            }
                            console.log("‚èπÔ∏è Stopping media recorder...");

                            mediaRecorder.stop();
                        }
                    }

                    if (Date.now() - lastSoundTime > 30000) {
                        if (isManualStop) return;
                        console.log("Maximum recording time reached");
                        isRecording = false;
                        if (silenceInterval) {
                            clearInterval(silenceInterval);
                            silenceInterval = null;
                        }
                        mediaRecorder.stop();
                    }
                }

                silenceInterval = setInterval(detectSilence, 200);

                // Failsafe: Force processing after extended speaking (15 seconds)
                setTimeout(() => {
                    if (isRecording && hasSound && mediaRecorder && mediaRecorder.state === 'recording') {
                        console.log("Failsafe: Force processing after 15 seconds of speaking");
                        isProcessingQuery = true;
                        isWaitingForResponse = true;
                        isRecording = false;
                        statusDiv.textContent = "ü§î Processing (timeout)...";

                        if (silenceInterval) {
                            clearInterval(silenceInterval);
                            silenceInterval = null;
                        }
                        mediaRecorder.stop();
                    }
                }, 15000);

                mediaRecorder.ondataavailable = e => audioChunks.push(e.data);

                mediaRecorder.onstop = async () => {
                    console.log("Recording stopped");

                    // CRITICAL: Check if this recorder is still current
                    if (mediaRecorder === null || mediaRecorder.onstop === null) {
                        console.log("‚ö†Ô∏è Stale onstop handler - ignoring");
                        return;
                    }

                    if (silenceInterval) {
                        clearInterval(silenceInterval);
                        silenceInterval = null;
                    }

                    if (isManualStop) {
                        console.log("Manual stop - not processing audio");
                        isProcessingQuery = false;
                        cleanupRecordingSession();
                        return;
                    }

                    // CRITICAL: Prevent recursive calls - but check AFTER manual stop
                    if (isProcessingQuery) {
                        console.log("Already processing, skipping onstop");
                        return;
                    }

                    // NOW set the flag for first-time execution
                    console.log("‚ö†Ô∏è Setting isProcessingQuery = true");
                    isProcessingQuery = true;
                    isWaitingForResponse = true;

                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    console.log("Audio blob size:", audioBlob.size, "hasSound:", hasSound);

                    if (!hasSound || audioBlob.size < 200) {
                        console.log("No meaningful audio detected");
                        statusDiv.textContent = "No speech detected. Try again...";
                        cleanupRecordingSession();
                        if (!isManualStop && !isProcessingQuery) {
                            setTimeout(() => {
                                if (!isProcessingQuery && !isBotSpeaking) {
                                    statusDiv.textContent = "üéôÔ∏è Listening...";
                                    startRecording();
                                }
                            }, 2000);
                        }
                    } else {
                        // isProcessingQuery already set at line 729 - don't set again!

                        const arrayBuffer = await audioBlob.arrayBuffer();
                        const uint8Array = new Uint8Array(arrayBuffer);

                        // Convert to base64 in chunks to avoid stack overflow
                        let binary = '';
                        const chunkSize = 8192;
                        for (let i = 0; i < uint8Array.length; i += chunkSize) {
                            const chunk = uint8Array.subarray(i, i + chunkSize);
                            binary += String.fromCharCode(...chunk);
                        }
                        const base64Audio = btoa(binary);

                        if (ws && ws.readyState === WebSocket.OPEN) {
                            console.log("üì§ Sending audio to server - size:", base64Audio.length, "session:", sessionId);

                            // Set processing status ONLY when actually sending
                            statusDiv.textContent = "ü§ñ Processing your question...";

                            ws.send(JSON.stringify({ audio: base64Audio, session_id: sessionId }));
                            console.log("‚úÖ Audio sent successfully");
                            statusDiv.textContent = "ü§ñ Thinking...";

                            // Enhanced timeout with auto-recovery
                            window.processingTimeout = setTimeout(() => {
                                if (isProcessingQuery && !isBotSpeaking) {
                                    console.warn("‚ö†Ô∏è Processing timeout (15s) - auto-recovering");
                                    isProcessingQuery = false;
                                    isWaitingForResponse = false;
                                    isBotSpeaking = false;
                                    cleanupRecordingSession();
                                    statusDiv.textContent = "‚è∞ Timeout - Auto-restarting...";
                                    micBtn.style.display = 'inline-flex';
                                    stopBtn.style.display = 'none';

                                    // Auto-restart recording after timeout (with safeguards)
                                    setTimeout(() => {
                                        if (!isProcessingQuery && !isBotSpeaking && !isManualStop && !isRecording) {
                                            console.log("Auto-restarting recording after timeout");
                                            statusDiv.textContent = "üéôÔ∏è Ready for your question...";
                                            try {
                                                startRecording();
                                            } catch (e) {
                                                console.error("Error restarting recording:", e);
                                                statusDiv.textContent = "‚ùå Recording restart failed";
                                            }
                                        }
                                    }, 1500);
                                }
                            }, 15000); // Increased to 15 seconds
                        } else {
                            console.log("WebSocket not connected");
                            cleanupRecordingSession();
                            statusDiv.textContent = "üîå Connection lost!";
                            isProcessingQuery = false;
                        }
                    }
                };

                if (isManualStop) {
                    console.log("Manual stop detected, aborting start");
                    cleanupRecordingSession(true);
                    return;
                }

                mediaRecorder.start();
                micBtn.style.display = 'none';
                stopBtn.style.display = 'inline-flex';
                if (!isManualStop) {
                    statusDiv.textContent = "üéôÔ∏è Listening... (Start speaking)";
                }

            } catch (err) {
                console.error("Microphone error:", err);

                // Handle specific error cases after interrupt
                if (err.name === 'NotAllowedError') {
                    statusDiv.textContent = "Microphone access denied! Please allow and try again.";
                } else if (err.name === 'NotFoundError') {
                    statusDiv.textContent = "No microphone found! Check your device.";
                } else if (err.name === 'AbortError') {
                    statusDiv.textContent = "Microphone was interrupted. Click to restart.";
                    // Auto-retry after abort error (common after interrupt)
                    setTimeout(() => {
                        if (!isManualStop && !isProcessingQuery) {
                            console.log("Auto-retrying after abort error");
                            startRecording();
                        }
                    }, 2000);
                } else {
                    statusDiv.textContent = "Microphone error! Click to retry.";
                }

                // Reset state on error
                isProcessingQuery = false;
                isWaitingForResponse = false;
                cleanupRecordingSession(false, true);
            }
        }

        micBtn.onclick = async () => {
            console.log("üéôÔ∏è Mic button clicked - Current state:", {
                isManualStop,
                isBotSpeaking,
                isProcessingQuery,
                wsState: ws?.readyState
            });

            isManualStop = false;

            // PRIORITY 1: Handle bot speaking interruption
            if (isBotSpeaking) {
                console.log("========== USER MANUALLY INTERRUPTED BOT ==========");

                // IMMEDIATELY clear all blocking flags
                isBotSpeaking = false;
                isProcessingQuery = false;
                isWaitingForResponse = false; // Clear waiting flag on interruption
                console.log("‚úÖ Cleared all flags for manual interruption");                // Stop voice interruption detection
                stopVoiceInterruptionDetection();

                // Stop bot audio and speech
                if (botAudio && !botAudio.paused) {
                    botAudio.pause();
                    botAudio.currentTime = 0;
                }
                window.speechSynthesis.cancel();

                // Remove visual indicators
                micBtn.classList.remove("interrupt-ready");

                // Fade last message
                const existingMessages = chatMessages.querySelectorAll('.message.assistant');
                if (existingMessages.length > 0) {
                    const lastMessage = existingMessages[existingMessages.length - 1];
                    lastMessage.style.opacity = '0.6';
                }

                // Cleanup any existing recording
                cleanupRecordingSession(true); // Force reset all flags

                statusDiv.textContent = "üéôÔ∏è Interrupted - Starting fresh recording...";

                if (ws && ws.readyState === WebSocket.OPEN) {
                    setTimeout(() => {
                        console.log("Starting fresh recording after manual interruption");
                        // Ensure clean state before restart
                        isManualStop = false;
                        isProcessingQuery = false;
                        isWaitingForResponse = false;
                        startRecording();
                    }, 800); // Increased delay for better stability
                }
                return;
            }

            // PRIORITY 2: Handle processing interruption
            if (isProcessingQuery || isWaitingForResponse) {
                console.log("========== INTERRUPTING PROCESSING/WAITING ==========");
                isProcessingQuery = false;
                isWaitingForResponse = false; // Clear waiting flag

                // Clear processing timeout
                if (window.processingTimeout) {
                    clearTimeout(window.processingTimeout);
                    window.processingTimeout = null;
                }

                cleanupRecordingSession(true); // Force reset all flags
                statusDiv.textContent = "üéôÔ∏è Interrupted - Ready for new question";

                if (ws && ws.readyState === WebSocket.OPEN) {
                    setTimeout(() => {
                        startRecording();
                    }, 500);
                }
                return;
            }

            // PRIORITY 3: Check if already connected
            if (ws && ws.readyState === WebSocket.OPEN) {
                console.log("WebSocket already connected, starting recording directly");
                startRecording();
                return;
            }

            if (ws && ws.readyState === WebSocket.CONNECTING) {
                console.log("WebSocket already connecting");
                return;
            }

            micBtn.disabled = true;
            micBtn.classList.add("recording");
            statusDiv.textContent = "üîÑ Connecting...";

            try {
                // Auto-detect protocol for WebSocket connection
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                ws = new WebSocket(`${protocol}//${window.location.host}/ws/voice`);

                ws.onopen = () => {
                    console.log("WS Connected");
                    statusDiv.textContent = "‚è≥ Waiting for session...";
                };

                ws.onmessage = async (event) => {
                    const data = JSON.parse(event.data);

                    if (data.session_id) {
                        sessionId = data.session_id;
                        console.log("Session started:", sessionId);
                        statusDiv.textContent = "üéôÔ∏è Ready! Start speaking...";
                        micBtn.disabled = false;
                        micBtn.classList.remove("recording");
                        startRecording();
                        return;
                    }

                    if (data.audio && data.bot_text) {
                        console.log("Received bot response");
                        // IMMEDIATELY clear all processing flags to unblock
                        isProcessingQuery = false;
                        isWaitingForResponse = false; // Clear waiting flag when response received
                        console.log("‚úÖ Cleared all processing flags - received bot response");

                        // Clear processing timeout
                        if (window.processingTimeout) {
                            clearTimeout(window.processingTimeout);
                            window.processingTimeout = null;
                        }

                        if (!isManualStop) {
                            statusDiv.textContent = "ü§ñ Speaking... (Click mic to interrupt)";
                        }

                        addMessage(data.bot_text, 'assistant');

                        const audioBlob = new Blob(
                            [Uint8Array.from(atob(data.audio), c => c.charCodeAt(0))],
                            { type: "audio/wav" }
                        );
                        botAudio.src = URL.createObjectURL(audioBlob);
                        isBotSpeaking = true;

                        // Start background voice detection for automatic interruption
                        if (!isManualStop) {
                            startVoiceInterruptionDetection();
                        }

                        // Visual indicator that interruption is available
                        micBtn.classList.add("interrupt-ready");
                        console.log("Bot started speaking, interruption enabled");

                        // Play audio with error handling
                        botAudio.play().catch(err => {
                            console.error("Bot audio play error:", err);
                            isProcessingQuery = false;
                            isBotSpeaking = false;
                            stopVoiceInterruptionDetection();
                            micBtn.classList.remove("interrupt-ready");
                            if (!isManualStop) {
                                statusDiv.textContent = "‚ö†Ô∏è Audio error. Click mic to continue";
                            }
                        });

                        botAudio.onended = () => {
                            console.log("Bot audio ended");
                            isBotSpeaking = false;
                            isProcessingQuery = false;

                            // Stop background voice interruption detection
                            stopVoiceInterruptionDetection();

                            // Remove interrupt visual indicator
                            micBtn.classList.remove("interrupt-ready");

                            if (!isManualStop) {
                                statusDiv.textContent = "üéôÔ∏è Listening...";
                                setTimeout(() => {
                                    startRecording();
                                }, 1000);
                            } else {
                                statusDiv.textContent = "Connection closed. Click mic to start.";
                            }
                        };

                        botAudio.onerror = () => {
                            console.log("Bot audio error");
                            isBotSpeaking = false;
                            isProcessingQuery = false;

                            // Stop background voice interruption detection
                            stopVoiceInterruptionDetection();

                            // Remove interrupt visual indicator
                            micBtn.classList.remove("interrupt-ready");

                            if (!isManualStop) {
                                statusDiv.textContent = "üéôÔ∏è Audio error. Listening again...";
                                setTimeout(() => {
                                    startRecording();
                                }, 1000);
                            } else {
                                statusDiv.textContent = "Connection closed. Click mic to start.";
                            }
                        };
                    }

                    if (data.bot_text && !data.audio) {
                        console.log("Received text-only response");
                        isProcessingQuery = false;
                        isBotSpeaking = false;
                        console.log("‚úÖ Cleared flags for text-only response");
                        addMessage(data.bot_text, 'assistant');
                        speakText(data.bot_text);
                        if (!isManualStop) {
                            setTimeout(() => {
                                statusDiv.textContent = "üéôÔ∏è Listening...";
                                startRecording();
                            }, 3000);
                        } else {
                            statusDiv.textContent = "Connection closed. Click mic to start.";
                        }
                    }
                };

                ws.onerror = (err) => {
                    console.error("WS Error:", err);
                    cleanupRecordingSession();
                    statusDiv.textContent = "‚ùå Connection error!";
                    micBtn.disabled = false;
                    isProcessingQuery = false;
                    isBotSpeaking = false;
                };

                ws.onclose = () => {
                    console.log("WS Closed");
                    cleanupRecordingSession();
                    statusDiv.textContent = "üîå Connection lost! Click to reconnect.";
                    micBtn.disabled = false;
                    isProcessingQuery = false;
                    isBotSpeaking = false;
                };

            } catch (error) {
                console.error("Failed to create WebSocket:", error);
                statusDiv.textContent = "‚ùå Failed to connect!";
                micBtn.disabled = false;
                micBtn.classList.remove("recording");
            }
        };

        stopBtn.onclick = () => {
            console.log("========== STOP BUTTON CLICKED ==========");

            // Set flags FIRST before any cleanup - ATOMIC OPERATION
            isManualStop = true;
            isProcessingQuery = false;
            isBotSpeaking = false;

            // Update UI immediately so user sees instant feedback
            statusDiv.textContent = "Stopping...";

            // Clear interval immediately to stop detectSilence - CRITICAL
            if (silenceInterval) {
                clearInterval(silenceInterval);
                silenceInterval = null;
                console.log("Silence interval cleared");
            }

            // Stop mediaRecorder directly without triggering cleanup chain
            if (mediaRecorder) {
                // Remove the onstop handler to prevent it from executing
                mediaRecorder.onstop = null;
                if (mediaRecorder.state !== "inactive") {
                    try {
                        mediaRecorder.stop();
                    } catch (e) {
                        console.log("Error stopping mediaRecorder:", e);
                    }
                }
                mediaRecorder = null;
            }

            // Stop all audio stream tracks
            if (currentStream) {
                currentStream.getTracks().forEach(track => {
                    track.stop();
                    console.log("Stopped track:", track.kind);
                });
                currentStream = null;
            }

            // Close audio context
            if (currentAudioContext && currentAudioContext.state !== 'closed') {
                currentAudioContext.close();
                currentAudioContext = null;
            }

            // Stop bot audio if playing
            if (botAudio && !botAudio.paused) {
                botAudio.pause();
                botAudio.currentTime = 0;
            }

            // Stop speech synthesis
            window.speechSynthesis.cancel();

            // Close WebSocket
            if (ws) {
                if (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING) {
                    try {
                        ws.send(JSON.stringify({ stop: true, session_id: sessionId }));
                    } catch (e) {
                        console.log("Error sending stop message:", e);
                    }
                    ws.close();
                }
                ws = null;
            }

            // Clear audio chunks
            audioChunks = [];
            sessionId = null;

            // Update UI
            micBtn.disabled = false;
            micBtn.classList.remove('recording');
            micBtn.style.display = 'inline-flex';
            stopBtn.style.display = 'none';

            // Final status update - use setTimeout to ensure it's the last thing
            setTimeout(() => {
                if (isManualStop) {
                    statusDiv.textContent = "Connection closed. Click mic to start.";
                }
            }, 100);

            console.log("========== ALL CLEANUP COMPLETED ==========");
        };
    </script>

</body>

</html>