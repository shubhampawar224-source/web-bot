<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Agent (Echo Proof)</title>
    <style>
        body {
            font-family: 'Segoe UI', sans-serif;
            background-color: #f0f2f5;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
        }

        .main-container {
            width: 100%;
            max-width: 450px;
            height: 90vh;
            background: white;
            border-radius: 20px;
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1);
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        .header {
            background: #007bff;
            color: white;
            padding: 15px;
            text-align: center;
            font-weight: 600;
        }

        .chat-box {
            flex: 1;
            padding: 20px;
            overflow-y: auto;
            background: #fff;
            display: flex;
            flex-direction: column;
            gap: 15px;
            scroll-behavior: smooth;
        }

        .message {
            max-width: 80%;
            padding: 10px 15px;
            border-radius: 15px;
            font-size: 0.95rem;
            line-height: 1.4;
            animation: fadeIn 0.3s ease;
            word-wrap: break-word;
        }

        .message.user {
            align-self: flex-end;
            background-color: #007bff;
            color: white;
            border-bottom-right-radius: 2px;
            text-align: right;
        }

        .message.assistant {
            align-self: flex-start;
            background-color: #e4e6eb;
            color: #050505;
            border-bottom-left-radius: 2px;
            text-align: left;
        }

        .controls {
            padding: 20px;
            background: #f8f9fa;
            display: flex;
            flex-direction: column;
            align-items: center;
            border-top: 1px solid #ddd;
        }

        .status-text {
            margin-bottom: 15px;
            font-size: 0.9rem;
            color: #555;
            height: 20px;
        }

        .mic-button {
            width: 70px;
            height: 70px;
            border-radius: 50%;
            background-color: #007bff;
            border: none;
            cursor: pointer;
            display: flex;
            justify-content: center;
            align-items: center;
            box-shadow: 0 4px 15px rgba(0, 123, 255, 0.3);
            transition: all 0.2s ease;
        }

        .mic-button.recording {
            background-color: #dc3545;
            box-shadow: 0 0 0 5px rgba(220, 53, 69, 0.2);
        }

        /* Visual indicator when AI is speaking (Mic Disabled) */
        .mic-button.listening-blocked {
            background-color: #6c757d;
            cursor: not-allowed;
            opacity: 0.6;
        }

        .mic-button svg {
            width: 30px;
            height: 30px;
            fill: white;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
    </style>
</head>

<body>

    <div class="main-container">
        <div class="header">üéôÔ∏è AI Voice Agent</div>
        <div class="chat-box" id="chatBox"></div>
        <div class="controls">
            <div class="status-text" id="statusText">Ready</div>
            <button class="mic-button" id="micBtn">
                <svg viewBox="0 0 24 24">
                    <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z" />
                    <path
                        d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z" />
                </svg>
            </button>
        </div>
    </div>

    <script>
        const chatBox = document.getElementById("chatBox");
        const statusText = document.getElementById("statusText");
        const micBtn = document.getElementById("micBtn");

        let ws;
        let audioCtx;     // For Mic
        let playbackCtx;  // For Speaker
        let micWorklet;
        let source;
        let stream;
        let isRecording = false;

        // Player State
        let nextStartTime = 0;
        let activeSources = [];

        // ‚≠ê NEW FLAG: AI Speaking State ‚≠ê
        let isAiSpeaking = false;

        const MIC_SAMPLE_RATE = 24000;

        micBtn.onclick = async () => {
            // Agar AI bol raha hai, toh click karne pe usse chup karao (Interruption)
            if (isAiSpeaking) {
                stopAudioPlayback();
                isAiSpeaking = false;
                micBtn.classList.remove("listening-blocked");
                statusText.textContent = "Listening...";
                return;
            }

            if (!isRecording) {
                await startConnection();
            } else {
                stopConnection();
            }
        };

        async function startConnection() {
            try {
                statusText.textContent = "Connecting...";

                playbackCtx = new (window.AudioContext || window.webkitAudioContext)();
                nextStartTime = 0;

                audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: MIC_SAMPLE_RATE });
                if (audioCtx.state === 'suspended') await audioCtx.resume();

                const protocol = window.location.protocol === "https:" ? "wss:" : "ws:";
                const wsUrl = `${protocol}//${window.location.host}/ws/voice`;
                ws = new WebSocket(wsUrl);

                ws.onopen = async () => {
                    statusText.textContent = "Connected. Starting Mic...";
                    micBtn.classList.add("recording");
                    isRecording = true;
                    await startMicrophone();
                    statusText.textContent = "Listening...";
                };

                ws.onmessage = async (event) => {
                    if (!isRecording) return;
                    const data = JSON.parse(event.data);

                    if (data.type === "transcript") addMessage(data.text, data.role);
                    if (data.type === "log") statusText.textContent = data.message;

                    if (data.type === "audio_chunk") {
                        statusText.textContent = "AI Speaking...";
                        handleAudioChunk(data.audio);
                    }
                };

                ws.onclose = () => { stopConnection(); statusText.textContent = "Disconnected"; };
                ws.onerror = (err) => { console.error("WS Error", err); statusText.textContent = "Connection Error"; }

            } catch (err) {
                console.error("Error:", err);
                statusText.textContent = "Error: " + err.message;
            }
        }

        function handleAudioChunk(base64Audio) {
            if (!playbackCtx || playbackCtx.state === 'closed') return;

            // ‚≠ê 1. Mark AI as Speaking
            isAiSpeaking = true;
            micBtn.classList.add("listening-blocked"); // Visual UI update

            const binaryString = window.atob(base64Audio);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) bytes[i] = binaryString.charCodeAt(i);

            const float32Data = new Float32Array(len / 2);
            const dataView = new DataView(bytes.buffer);

            for (let i = 0; i < float32Data.length; i++) {
                const int16 = dataView.getInt16(i * 2, true);
                float32Data[i] = int16 / 32768.0;
            }

            const audioBuffer = playbackCtx.createBuffer(1, float32Data.length, 24000);
            audioBuffer.getChannelData(0).set(float32Data);

            const source = playbackCtx.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(playbackCtx.destination);

            const currentTime = playbackCtx.currentTime;
            if (nextStartTime < currentTime) nextStartTime = currentTime + 0.1;

            source.start(nextStartTime);
            nextStartTime += audioBuffer.duration;

            activeSources.push(source);

            source.onended = () => {
                activeSources = activeSources.filter(s => s !== source);

                // ‚≠ê 2. Only Unmute Mic when ALL audio is finished
                if (activeSources.length === 0) {
                    isAiSpeaking = false;
                    micBtn.classList.remove("listening-blocked");
                    if (isRecording) statusText.textContent = "Listening...";
                }
            };
        }

        async function startMicrophone() {
            try {
                stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: MIC_SAMPLE_RATE,
                        echoCancellation: true, // Echo cancel is still good backup
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });

                source = audioCtx.createMediaStreamSource(stream);

                const micCode = `
                    class PCMProcessor extends AudioWorkletProcessor {
                        process(inputs, outputs) {
                            const input = inputs[0];
                            if (input.length > 0) {
                                this.port.postMessage(input[0]);
                            }
                            return true;
                        }
                    }
                    registerProcessor('pcm-processor', PCMProcessor);
                `;

                const blob = new Blob([micCode], { type: "text/javascript" });
                await audioCtx.audioWorklet.addModule(URL.createObjectURL(blob));

                micWorklet = new AudioWorkletNode(audioCtx, 'pcm-processor');

                let bufferAccumulator = [];

                micWorklet.port.onmessage = (e) => {
                    if (!isRecording || ws.readyState !== WebSocket.OPEN) return;

                    // ‚≠ê 3. CRITICAL LOGIC: BLOCK MIC WHEN AI IS SPEAKING ‚≠ê
                    if (isAiSpeaking) {
                        bufferAccumulator = []; // Clear user input while AI speaks
                        return; // Don't send anything!
                    }

                    const float32Chunk = e.data;
                    bufferAccumulator.push(...float32Chunk);

                    if (bufferAccumulator.length >= 4096) {
                        const chunkToSend = new Float32Array(bufferAccumulator);
                        bufferAccumulator = [];

                        const pcmData = floatTo16BitPCM(chunkToSend);
                        const base64Audio = arrayBufferToBase64(pcmData);
                        ws.send(JSON.stringify({ audio: base64Audio }));
                    }
                };

                source.connect(micWorklet);
                micWorklet.connect(audioCtx.destination);
            } catch (error) {
                console.error("Mic Error:", error);
            }
        }

        function stopConnection() {
            isRecording = false;
            isAiSpeaking = false; // Reset flag
            micBtn.classList.remove("recording");
            micBtn.classList.remove("listening-blocked");
            statusText.textContent = "Ready";

            if (ws) { ws.close(); ws = null; }
            stopAudioPlayback();

            if (playbackCtx) { playbackCtx.close(); playbackCtx = null; }
            if (audioCtx) { audioCtx.close(); audioCtx = null; }
            if (stream) { stream.getTracks().forEach(t => t.stop()); stream = null; }
        }

        function stopAudioPlayback() {
            activeSources.forEach(s => { try { s.stop(); } catch (e) { } });
            activeSources = [];
            if (playbackCtx) nextStartTime = playbackCtx.currentTime;

            // Allow mic again immediately
            isAiSpeaking = false;
            micBtn.classList.remove("listening-blocked");
        }

        function addMessage(text, role) {
            if (!text) return;
            const div = document.createElement("div");
            div.className = `message ${role}`;
            div.textContent = text;
            chatBox.appendChild(div);
            chatBox.scrollTo({ top: chatBox.scrollHeight, behavior: 'smooth' });
        }

        function floatTo16BitPCM(input) {
            const output = new Int16Array(input.length);
            for (let i = 0; i < input.length; i++) {
                const s = Math.max(-1, Math.min(1, input[i]));
                output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return output.buffer;
        }

        function arrayBufferToBase64(buffer) {
            let binary = '';
            const bytes = new Uint8Array(buffer);
            const len = bytes.byteLength;
            for (let i = 0; i < len; i++) { binary += String.fromCharCode(bytes[i]); }
            return window.btoa(binary);
        }
    </script>
</body>

</html>