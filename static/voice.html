<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>üöÄ Robust Voice AI</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            background: #fff;
            color: #222;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            min-height: 100vh;
        }

        .container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            padding: 20px;
        }

        .chat-window {
            width: 100%;
            max-width: 500px;
            background: #fff;
            border-radius: 24px;
            box-shadow: 0 4px 32px rgba(0, 0, 0, 0.08);
            overflow: hidden;
        }

        .chat-header {
            background: #2563eb;
            color: #fff;
            text-align: center;
            padding: 15px 0 10px 0;
            font-size: 2rem;
            font-weight: bold;
            border-radius: 24px 24px 0 0;
            letter-spacing: 1px;
        }

        .chat-messages {
            padding: 24px;
            height: 250px;
            overflow-y: auto;
            background: #fff;
            display: flex;
            flex-direction: column;
        }

        .message {
            padding: 18px 22px;
            border-radius: 16px;
            margin-bottom: 14px;
            font-size: 1.1rem;
            line-height: 1.6;
            background: #f3f6fa;
            color: #222;
            border: 1px solid #e5e7eb;
            max-width: 75%;
            word-break: break-word;
        }

        .message.user {
            background: #e0e7ff;
            color: #222;
            align-self: flex-end;
            border: 1px solid #c7d2fe;
            margin-left: auto;
            margin-right: 0;
        }

        .message.assistant {
            background: #f3f6fa;
            color: #222;
            align-self: flex-start;
            border: 1px solid #e5e7eb;
            margin-right: auto;
            margin-left: 0;
        }

        .status-bar {
            background: #f3f6fa;
            color: #2563eb;
            text-align: center;
            padding: 10px;
            font-size: 1rem;
        }

        .bottom-bar {
            background: #fff;
            padding: 18px 24px 24px 24px;
            display: flex;
            align-items: center;
            justify-content: space-between;
            position: relative;
        }

        .input-bar {
            display: flex;
            align-items: center;
            width: 100%;
        }

        .input-box {
            flex: 1;
            padding: 14px 18px;
            border: 1px solid #e5e7eb;
            border-radius: 12px;
            background: #f3f6fa;
            color: #222;
            font-size: 1.1rem;
            margin-right: 12px;
        }

        .input-box::placeholder {
            color: #94a3b8;
            opacity: 1;
        }

        .mic-btn {
            width: 54px;
            height: 54px;
            border-radius: 50%;
            border: none;
            background: linear-gradient(135deg, #3b4cca 0%, #5a6ee6 100%);
            color: #fff;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: background 0.3s, box-shadow 0.3s;
            box-shadow: 0 4px 16px rgba(42, 57, 144, 0.15);
            font-size: 1.5rem;
            margin-right: 0;
        }

        .mic-btn:hover {
            background: linear-gradient(135deg, #5a6ee6 0%, #3b4cca 100%);
            box-shadow: 0 6px 24px rgba(42, 57, 144, 0.18);
        }

        .stop-btn {
            background: #ef5350;
            display: none;
        }

        .stop-btn:hover {
            background: #d32f2f;
        }

        @keyframes pulse {
            0% {
                opacity: 1;
            }

            50% {
                opacity: 0.5;
            }

            100% {
                opacity: 1;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="chat-window">
            <div class="chat-header">AI voice assistant</div>
            <div class="chat-messages" id="chat-messages"></div>
            <div class="status-bar" id="status">Ready to connect</div>
            <div class="bottom-bar">
                <div class="input-bar">
                    <input id="textInput" class="input-box" type="text" placeholder="Start speaking..." readonly />
                    <button id="micBtn" class="mic-btn">üéôÔ∏è</button>
                    <button id="stopBtn" class="stop-btn">‚èπÔ∏è</button>
                </div>
            </div>
        </div>
    </div>

    <script>
        // --- UI Elements ---
        const statusDiv = document.getElementById('status');
        const chatMessages = document.getElementById('chat-messages');
        const micBtn = document.getElementById('micBtn');
        const stopBtn = document.getElementById('stopBtn');

        // --- Core State ---
        let ws = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let sessionId = null;

        // --- Flags ---
        let isProcessingQuery = false;
        let isBotSpeaking = false;
        let isWaitingForResponse = false;
        let isManualStop = false;
        let isRecording = false;

        // --- Audio Context & Streams ---
        let currentStream = null;
        let recordingContext = null;
        let audioCtx = null;
        let nextStartTime = 0;
        const BUFFER_AHEAD_TIME = 0.5;

        // --- Detection Vars ---
        let silenceInterval = null;
        let voiceInterruptionStream = null;
        let voiceInterruptionContext = null;
        let voiceInterruptionAnalyser = null;
        let voiceInterruptionInterval = null;

        function addMessage(text, sender) {
            const div = document.createElement('div');
            div.className = `message ${sender}`;
            div.textContent = text;
            chatMessages.appendChild(div);
            chatMessages.scrollTop = chatMessages.scrollHeight;
        }

        // ==========================================
        // üî• THE FIX: CENTRALIZED DISCONNECT LOGIC
        // ==========================================
        function handleConnectionLost(reason) {
            console.log("üõë Connection Reset Triggered:", reason);

            // 1. Flags Reset (Sabse Pehle)
            isManualStop = true;
            isRecording = false;
            isBotSpeaking = false;
            isProcessingQuery = false;
            isWaitingForResponse = false;

            // 2. Kill Audio & Mic
            stopAudioPlayback();
            cleanupRecordingSession(true);

            // 3. Close Socket if open
            if (ws) {
                ws.onclose = null; // Prevent loop
                ws.onerror = null;
                if (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING) {
                    ws.close();
                }
                ws = null;
            }

            // 4. Force UI Reset (Ye "Speaking..." ko hata dega)
            statusDiv.textContent = "Disconnected. Click mic to connect.";
            statusDiv.style.color = "red"; // Red color to indicate stopped

            stopBtn.style.display = 'none';
            micBtn.style.display = 'inline-flex';
            micBtn.classList.remove('recording', 'interrupt-ready');
            micBtn.disabled = false;
        }

        // --- AUDIO PLAYBACK ---
        function initAudioContext() {
            if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            if (audioCtx.state === 'suspended') audioCtx.resume();
        }

        async function playAudioChunk(base64Data) {
            if (isManualStop) return; // Agar stop daba diya to audio mat chalao

            initAudioContext();
            const binaryString = atob(base64Data);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) bytes[i] = binaryString.charCodeAt(i);

            try {
                const buffer = await audioCtx.decodeAudioData(bytes.buffer);
                scheduleBuffer(buffer);
            } catch (e) { console.error("Decode error", e); }
        }

        function scheduleBuffer(buffer) {
            if (isManualStop) return;

            const source = audioCtx.createBufferSource();
            source.buffer = buffer;
            source.connect(audioCtx.destination);

            const now = audioCtx.currentTime;
            if (nextStartTime < now) nextStartTime = now + BUFFER_AHEAD_TIME;

            source.start(nextStartTime);
            nextStartTime += buffer.duration;

            isBotSpeaking = true;
            micBtn.classList.add("interrupt-ready");

            // Speaking status sirf tab dikhana jab actually connect ho
            if (!isManualStop) statusDiv.textContent = "ü§ñ Speaking...";
        }

        function stopAudioPlayback() {
            if (audioCtx) {
                audioCtx.close().then(() => { audioCtx = null; nextStartTime = 0; });
            }
            isBotSpeaking = false;
        }

        function speakText(text) {
            window.speechSynthesis.cancel();
            const u = new SpeechSynthesisUtterance(text);
            window.speechSynthesis.speak(u);
        }

        // --- INTERRUPTION LOGIC ---
        async function startInterruptionDetection() {
            if (voiceInterruptionStream) return;
            try {
                voiceInterruptionStream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: true, noiseSuppression: true } });
                voiceInterruptionContext = new AudioContext();
                const source = voiceInterruptionContext.createMediaStreamSource(voiceInterruptionStream);
                voiceInterruptionAnalyser = voiceInterruptionContext.createAnalyser();
                voiceInterruptionAnalyser.fftSize = 2048;
                source.connect(voiceInterruptionAnalyser);

                const dataArray = new Float32Array(voiceInterruptionAnalyser.frequencyBinCount);
                let consecutive = 0;

                voiceInterruptionInterval = setInterval(() => {
                    if (!isBotSpeaking || isManualStop) return;
                    voiceInterruptionAnalyser.getFloatTimeDomainData(dataArray);
                    const rms = Math.sqrt(dataArray.reduce((s, v) => s + v * v, 0) / dataArray.length);
                    const db = 20 * Math.log10(rms + 1e-8);

                    if (db > -30) {
                        consecutive++;
                        if (consecutive >= 3) handleInterruption();
                    } else {
                        consecutive = Math.max(0, consecutive - 1);
                    }
                }, 200);
            } catch (e) { console.error(e); }
        }

        function stopInterruptionDetection() {
            if (voiceInterruptionInterval) clearInterval(voiceInterruptionInterval);
            if (voiceInterruptionStream) voiceInterruptionStream.getTracks().forEach(t => t.stop());
            if (voiceInterruptionContext) voiceInterruptionContext.close();
            voiceInterruptionStream = null;
        }

        function handleInterruption() {
            console.log("Interruption triggered");
            isBotSpeaking = false;
            stopAudioPlayback();
            stopInterruptionDetection();
            window.speechSynthesis.cancel();
            micBtn.classList.remove("interrupt-ready");

            cleanupRecordingSession(false);
            if (!isManualStop && ws && ws.readyState === WebSocket.OPEN) {
                statusDiv.textContent = "Interrupted! Listening...";
                setTimeout(startRecording, 500);
            }
        }

        function cleanupRecordingSession(fullStop = false) {
            stopInterruptionDetection();
            if (silenceInterval) clearInterval(silenceInterval);
            if (recordingContext) recordingContext.close();

            if (mediaRecorder && mediaRecorder.state !== "inactive") {
                if (fullStop) mediaRecorder.onstop = null;
                mediaRecorder.stop();
            }
            if (currentStream) currentStream.getTracks().forEach(t => t.stop());
            recordingContext = null;
            audioChunks = [];
        }

        // --- RECORDING ---
        // --- IMPROVED RECORDING LOGIC ---
        async function startRecording() {
            if (isManualStop || !ws || ws.readyState !== WebSocket.OPEN) return;
            
            cleanupRecordingSession(false);
            if (isBotSpeaking) handleInterruption();

            try {
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: { 
                        echoCancellation: true, 
                        noiseSuppression: true,
                        autoGainControl: true // Added AGC for better volume
                    } 
                });
                currentStream = stream;
                recordingContext = new AudioContext();
                const source = recordingContext.createMediaStreamSource(stream);
                const analyser = recordingContext.createAnalyser();
                analyser.fftSize = 2048; // Smaller FFT for faster reaction
                source.connect(analyser);

                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];
                
                let hasSound = false;
                let silenceCount = 0;
                let recordingStartTime = Date.now(); // Track start time
                isRecording = true;
                
                const dataArray = new Float32Array(analyser.frequencyBinCount);

                // --- NEW SILENCE DETECTION LOGIC ---
                silenceInterval = setInterval(() => {
                    if (!isRecording) return;
                    
                    analyser.getFloatTimeDomainData(dataArray);
                    
                    // Calculate RMS (Volume Level)
                    let sum = 0;
                    for (let i = 0; i < dataArray.length; i++) {
                        sum += dataArray[i] * dataArray[i];
                    }
                    const rms = Math.sqrt(sum / dataArray.length);
                    const db = 20 * Math.log10(rms + 1e-8); // Convert to decibels

                    // CHANGE 1: Threshold lowered to -60 (More sensitive)
                    // Allows quieter speech to be detected as "Voice"
                    if (db > -60) { 
                        if (!hasSound) console.log("üó£Ô∏è Voice detected");
                        hasSound = true;
                        silenceCount = 0;
                        statusDiv.textContent = "üéôÔ∏è Listening...";
                        statusDiv.style.color = "#ef5350"; // Red for recording
                    } 
                    else if (hasSound) {
                        // Check if minimum recording time (1.5s) has passed
                        // This prevents cutting off if you start with a pause
                        if (Date.now() - recordingStartTime > 1500) {
                            silenceCount++;
                            
                            // CHANGE 2: Wait longer (12 * 200ms = 2.4 seconds)
                            if (silenceCount > 12) { 
                                console.log("ü§´ Silence detected, stopping...");
                                isRecording = false;
                                mediaRecorder.stop();
                            }
                        }
                    }
                }, 200); // Check every 200ms

                // Max duration safety (30 seconds)
                setTimeout(() => { if (isRecording) mediaRecorder.stop(); }, 30000);

                mediaRecorder.ondataavailable = e => {
                    if (e.data.size > 0) audioChunks.push(e.data);
                };

                mediaRecorder.onstop = () => {
                    if (isManualStop) return;
                    
                    // If audio is too short, ignore
                    if (!hasSound || audioChunks.length === 0) {
                        console.log("No meaningful audio, restarting...");
                        cleanupRecordingSession();
                        setTimeout(startRecording, 500);
                        return;
                    }
                    
                    isProcessingQuery = true;
                    statusDiv.textContent = "ü§ñ Thinking...";
                    statusDiv.style.color = "#4a90e2";

                    const blob = new Blob(audioChunks, { type: 'audio/wav' });
                    const reader = new FileReader();
                    reader.readAsDataURL(blob);
                    reader.onloadend = () => {
                        const base64 = reader.result.split(',')[1];
                        if (ws && ws.readyState === WebSocket.OPEN) {
                            // Send to backend
                            ws.send(JSON.stringify({ audio: base64, session_id: sessionId }));
                        }
                    };
                };

                mediaRecorder.start(500); // Request data every 500ms to ensure buffer isn't empty
                micBtn.style.display = 'none';
                stopBtn.style.display = 'inline-flex';
                statusDiv.textContent = "üéôÔ∏è Listening...";

            } catch (e) {
                console.error(e);
                statusDiv.textContent = "Mic Error";
                cleanupRecordingSession(true);
                handleConnectionLost("Mic Failed");
            }
        }
        // --- WEBSOCKET SETUP ---
        function initWebSocket() {
            micBtn.disabled = true;
            statusDiv.textContent = "üîÑ Connecting...";
            statusDiv.style.color = "#666";

            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            ws = new WebSocket(`${protocol}//${window.location.host}/ws/voice`);

            ws.onopen = () => statusDiv.textContent = "‚è≥ Authenticating...";

            ws.onmessage = (event) => {
                if (isManualStop) return; // Ignore messages if stopped

                const data = JSON.parse(event.data);

                if (data.session_id) {
                    sessionId = data.session_id;
                    statusDiv.textContent = "üéôÔ∏è Ready!";
                    micBtn.disabled = false;
                    startRecording();
                    return;
                }

                if (isWaitingForResponse) {
                    isWaitingForResponse = false;
                    isProcessingQuery = false;
                    if (window.processingTimeout) clearTimeout(window.processingTimeout);
                }

                if (data.type === 'text_start') {
                    addMessage(data.bot_text, 'assistant');
                    if (data.user_text) addMessage(data.user_text, 'user');
                    statusDiv.textContent = "ü§ñ Speaking...";
                    if (!isManualStop) startInterruptionDetection();
                } else if (data.type === 'audio_chunk') {
                    playAudioChunk(data.audio);
                } else if (data.audio && !data.type) {
                    addMessage(data.bot_text, 'assistant');
                    playAudioChunk(data.audio);
                } else if (data.type === 'error') {
                    statusDiv.textContent = "Error: " + data.message;
                }
            };

            // Ensures UI resets on close/error
            ws.onerror = () => handleConnectionLost("Socket Error");
            ws.onclose = () => handleConnectionLost("Socket Closed");
        }

        // --- BUTTONS ---
        micBtn.onclick = () => {
            isManualStop = false;
            if (isBotSpeaking) {
                handleInterruption();
                return;
            }
            // Connect logic
            if (!ws || ws.readyState !== WebSocket.OPEN) {
                initWebSocket();
            } else {
                startRecording();
            }
        };

        stopBtn.onclick = () => {
            // Send stop to server just in case
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ stop: true }));
            }
            // Call the centralized handler
            handleConnectionLost("Manual Stop Button");
        };
    </script>
</body>

</html>